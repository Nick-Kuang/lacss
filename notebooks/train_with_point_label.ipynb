{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kks5J-TD8-Bh"
      },
      "source": [
        "# LACSS Point-supervised Training Demo\n",
        "\n",
        "The demo will train a model to segment microscopy images of cells, using only point label.\n",
        "\n",
        " * The point label was produced automatically from DAPI images\n",
        "\n",
        "We will go through these steps:\n",
        "\n",
        "- Setup the data pipeline\n",
        "\n",
        "- Initialize a model trainer\n",
        "\n",
        "- Perform model training\n",
        "\n",
        "- Visualize the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp1Y6zHl9ddY"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ivh9LzC89QK"
      },
      "outputs": [],
      "source": [
        "!pip install lacss\n",
        "\n",
        "import imageio\n",
        "import json\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "from skimage.color import label2rgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import lacss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr0QliBABDOh"
      },
      "source": [
        "## Data pipeline\n",
        "\n",
        "Lacss expect training data from a python generator that produces the following data:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"image\": ndarray[B, W, H, C],\n",
        "  \"gt_locations\": ndarray[B, N, 2]\n",
        "},\n",
        "```\n",
        "\n",
        "Here we will set up the data pipeline using tensorflow.dataset library, which has many useful utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqdox1oOccv4"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget -c https://data.mendeley.com/public-files/datasets/89s3ymz5wn/files/f976856c-08c5-4bba-85a7-3881e0593115/file_downloaded -O A431.zip\n",
        "\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from os.path import join\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "data_path = 'image_data'\n",
        "with zipfile.ZipFile('A431.zip', \"r\") as f:\n",
        "    f.extractall(data_path)\n",
        "\n",
        "# show an example of the training data\n",
        "img = imageio.imread(join(data_path, 'train', 'img_0000.tif'))\n",
        "with open(join(data_path, \"train.json\")) as f:\n",
        "    locations = json.load(f)\n",
        "\n",
        "lacss.utils.show_images([\n",
        "    img,\n",
        "    np.zeros_like(img),\n",
        "])\n",
        "\n",
        "ax = plt.gcf().get_axes()\n",
        "ax[0].set_title(\"Image\")\n",
        "for pos in locations[0]['locations']:\n",
        "    c = Circle((pos[1], pos[0]), radius=2, edgecolor='white')\n",
        "    ax[1].add_patch(c)\n",
        "ax[1].set_title(\"Label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3bSK8QDEKlM"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "# create a tensowflow dataset from the files on disk\n",
        "ds = lacss.data.dataset_from_simple_annotations(\n",
        "    join(data_path, \"train.json\"),\n",
        "    join(data_path, \"train\"),\n",
        "    image_shape=[None, None, 1]\n",
        ")\n",
        "\n",
        "def parser(data):\n",
        "\n",
        "    # build-in data augmentation function\n",
        "    data = lacss.data.parse_train_data_func(data, size_jitter=[0.8, 1.2])\n",
        "\n",
        "    # It is important to pad the locations tensor so that all elements of the dataset are of the same shape\n",
        "    locations = data['locations']\n",
        "    n_pad = 768 - len(locations)\n",
        "    locations = tf.pad(locations, [[0, n_pad], [0,0]], constant_values=-1)\n",
        "\n",
        "    return dict(\n",
        "      image = data['image'],\n",
        "      gt_locations = locations, \n",
        "    )\n",
        "\n",
        "ds = ds.map(parser).repeat().batch(batch_size)\n",
        "\n",
        "# make sure the dataset has the correct element structure\n",
        "ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ4ibyHzEUO7"
      },
      "outputs": [],
      "source": [
        "# Convert the td.dataset to generator\n",
        "train_gen = lacss.train.TFDatasetAdapter(ds, steps=-1).get_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GESuO6zM9tso"
      },
      "source": [
        "## Initialize a trainer\n",
        "\n",
        "We will use transfer learning by starting from a pre-trained model. Transfer learning is generally beneficial even if the orginal model was trained on data that looks very different from the current images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2mp9sJM-Tul"
      },
      "outputs": [],
      "source": [
        "# Load a pretrain model\n",
        "# This model was trained on bright field microscopy images (LIVECell dataset)\n",
        "!wget -c https://data.mendeley.com/public-files/datasets/sj3vrvm6w3/files/667af564-0242-4c1d-87b0-c46e6bc3f63d/file_downloaded -O cnsp4_lc.pkl\n",
        "\n",
        "from dataclasses import asdict\n",
        "from lacss.deploy import load_from_pretrained\n",
        "from flax.core.frozen_dict import freeze, unfreeze\n",
        "\n",
        "pretrained_module, pretrained_params = load_from_pretrained(\"cnsp4_lc.pkl\")\n",
        "\n",
        "# LacssWithHelper contains both the principal model and the collaborator model\n",
        "lacss_cfg = asdict(pretrained_module)\n",
        "model = lacss.modules.lacss.LacssWithHelper(\n",
        "    cfg=lacss_cfg,\n",
        "    aux_edge_cfg={},\n",
        "    aux_fg_cfg={\"conv_spec\": (16, 32)},\n",
        ")\n",
        "\n",
        "losses = [\n",
        "    lacss.losses.LPNLoss(), # detector head loss\n",
        "    lacss.losses.SelfSupervisedInstanceLoss(), # segmentation head loss\n",
        "    lacss.losses.AuxEdgeLoss(), # consistency loss\n",
        "    lacss.losses.AuxSegLoss( # consistency loss\n",
        "        offset_sigma=20.0,  # use bigger value for large cells\n",
        "        offset_scale=2.0,\n",
        "    ),\n",
        "    lacss.losses.AuxSizeLoss(), # prevent model collapse\n",
        "]\n",
        "\n",
        "trainer = lacss.train.Trainer(\n",
        "    model=model,\n",
        "    losses=losses,\n",
        "    optimizer=optax.adam(0.001),\n",
        "    strategy=lacss.train.strategy.VMapped,\n",
        "    seed=1234,\n",
        ")\n",
        "\n",
        "trainer.initialize(train_gen)\n",
        "\n",
        "# Merge with the pre-trained weights\n",
        "_, aux_edge_params = trainer.params.pop(\"_aux_edge_module\")\n",
        "_, aux_fg_params = trainer.params.pop(\"_aux_fg_module\")\n",
        "trainer.state = trainer.state.replace(\n",
        "    params = freeze(dict(\n",
        "        _lacss=pretrained_params,\n",
        "        _aux_edge_module=aux_edge_params,\n",
        "        _aux_fg_module=aux_fg_params,\n",
        "    ))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5qcbxs5aomk"
      },
      "source": [
        "## Training\n",
        "\n",
        "Trainer.train() function returns an iterator, stepping through which will drive the training of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09jrmOyjaoAs"
      },
      "outputs": [],
      "source": [
        "n_epoch = 5\n",
        "steps_per_epoch = 3000\n",
        "\n",
        "train_iter = trainer.train(train_gen, rng_cols=[\"droppath\", \"augment\"], training=True)\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  \n",
        "  print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "  for steps in tqdm(range(steps_per_epoch)):\n",
        "\n",
        "      logs = next(train_iter)\n",
        "\n",
        "  print(\", \".join([f\"{k}:{v:.4f}\" for k, v in logs.items()]))\n",
        "\n",
        "  # reset logs\n",
        "  trainer.reset()\n",
        "\n",
        "  # perform validation here\n",
        "\n",
        "  # maybe save a training checkpoint\n",
        "  # trainer.checkpoint(f\"cp-{epoch}\")\n",
        "\n",
        "# save the current model. We only need the principal model\n",
        "trainer.save_model(\"model.pkl\", \"_lacss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_tBseBbc-mw"
      },
      "source": [
        "## Visualize  the model prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOX43ZnYyX-"
      },
      "outputs": [],
      "source": [
        "image = imageio.imread(join(data_path, 'test', 'img_0001.tif'))\n",
        "gt = imageio.imread(join(data_path, 'test', 'masks_0001.tif'))\n",
        "\n",
        "# normalize\n",
        "img = image - image.mean()\n",
        "img /= img.std()\n",
        "img = img[..., None]\n",
        "\n",
        "# prediction\n",
        "model_output = trainer.model.apply(\n",
        "    dict(params=trainer.params),\n",
        "    image = img,\n",
        ")\n",
        "pred = lacss.ops.patches_to_label(model_output, input_size=img.shape[:2])\n",
        "pred = np.asarray(pred)\n",
        "\n",
        "lacss.utils.show_images([\n",
        "    img,\n",
        "    label2rgb(pred, bg_label=0),\n",
        "    label2rgb(gt, bg_label=0),\n",
        "])\n",
        "titles = ['Input', \"Prediction\", \"Ground Truth\"]\n",
        "[ax.set_title(title) for ax, title in zip(plt.gcf().get_axes(), titles)]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
