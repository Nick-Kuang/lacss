{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kks5J-TD8-Bh"
      },
      "source": [
        "# LACSS Weakly-supervised Training Demo\n",
        "\n",
        "The demo will train a model to segment microscopy images of cells, using point label + mask label.\n",
        "\n",
        " * The point label was produced automatically from DAPI images\n",
        "\n",
        " * The image-level mask label was produced manually.\n",
        "\n",
        "We will go through these steps:\n",
        "\n",
        "- Setup the data pipeline\n",
        "\n",
        "- Initialize a model trainer\n",
        "\n",
        "- Perform model training\n",
        "\n",
        "- Visualize the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp1Y6zHl9ddY"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ivh9LzC89QK"
      },
      "outputs": [],
      "source": [
        "!pip install lacss\n",
        "\n",
        "import imageio\n",
        "import json\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "from skimage.color import label2rgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "from flax.core.frozen_dict import freeze, unfreeze\n",
        "\n",
        "import lacss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr0QliBABDOh"
      },
      "source": [
        "## Data pipeline\n",
        "\n",
        "Lacss expect training data from a python generator that produces the following data:\n",
        "\n",
        "```\n",
        "x_data, y_data = (\n",
        "  {\n",
        "    \"image\": ndarray[B, W, H, C],\n",
        "    \"gt_locations\": ndarray[B, N, 2]\n",
        "  },\n",
        "  {\n",
        "    \"gt_mask\": ndarray[B, W, H]\n",
        "  }\n",
        ")\n",
        "```\n",
        "\n",
        "Here we will set up the data pipeline using tensorflow.dataset library, which has many useful utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqdox1oOccv4"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget -c https://data.mendeley.com/public-files/datasets/89s3ymz5wn/files/f976856c-08c5-4bba-85a7-3881e0593115/file_downloaded -O A431.zip\n",
        "\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from os.path import join\n",
        "from matplotlib.patches import Circle\n",
        "\n",
        "data_path = 'image_data'\n",
        "with zipfile.ZipFile('A431.zip', \"r\") as f:\n",
        "    f.extractall(data_path)\n",
        "\n",
        "# show an example of the training data\n",
        "img = imageio.imread(join(data_path, 'train', 'img_0000.tif'))\n",
        "mask = imageio.imread(join(data_path, 'train', 'masks_0000.tif'))\n",
        "with open(join(data_path, \"train.json\")) as f:\n",
        "    locations = json.load(f)\n",
        "\n",
        "lacss.utils.show_images([\n",
        "    img,\n",
        "    255 - mask,\n",
        "])\n",
        "\n",
        "ax = plt.gcf().get_axes()\n",
        "ax[0].set_title(\"Image\")\n",
        "for pos in locations[0]['locations']:\n",
        "    c = Circle((pos[1], pos[0]), radius=2, edgecolor='white')\n",
        "    ax[1].add_patch(c)\n",
        "ax[1].set_title(\"Label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3bSK8QDEKlM"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "# create a tensowflow dataset from the files on disk\n",
        "ds = lacss.data.dataset_from_simple_annotations(\n",
        "    join(data_path, \"train.json\"),\n",
        "    join(data_path, \"train\"),\n",
        "    image_shape=[None, None, 1]\n",
        ")\n",
        "\n",
        "def parser(data):\n",
        "\n",
        "    # build-in data augmentation function\n",
        "    data = lacss.data.parse_train_data_func(data, size_jitter=[0.8, 1.2])\n",
        "\n",
        "    # It is important to pad the locations tensor so that all elements of the dataset are of the same shape\n",
        "    locations = data['locations']\n",
        "    n_pad = 768 - len(locations)\n",
        "    locations = tf.pad(locations, [[0, n_pad], [0,0]], constant_values=-1)\n",
        "\n",
        "    return (\n",
        "        dict(\n",
        "            image = data['image'],\n",
        "            gt_locations = locations, \n",
        "        ),\n",
        "        dict(\n",
        "            gt_mask = data['binary_mask'],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "ds = ds.map(parser).repeat().batch(batch_size)\n",
        "\n",
        "# make sure the dataset has the correct element structure\n",
        "ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ4ibyHzEUO7"
      },
      "outputs": [],
      "source": [
        "# Convert the td.dataset to generator\n",
        "train_gen = lacss.train.TFDatasetAdapter(ds, steps=-1).get_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GESuO6zM9tso"
      },
      "source": [
        "## Initialize a trainer\n",
        "\n",
        "The idea is to co-train two models: a principal model and a collaborator model\n",
        "\n",
        "In addition, we will initialize the backbone of the pricipal model with ImageNet weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2mp9sJM-Tul"
      },
      "outputs": [],
      "source": [
        "# configuration for the principal model\n",
        "cfg_json = '''\n",
        "{\n",
        "  \"backbone\": \"ConvNeXt\",\n",
        "  \"backbone_cfg\": {\n",
        "  \t\"depths\": [3,3,27,3],\n",
        "  \t\"drop_path_rate\": 0.4\n",
        "  },\n",
        "  \"detector\": {\n",
        "    \"test_max_output\": 768,\n",
        "    \"train_max_output\": 768\n",
        "  },\n",
        "  \"segmentor\": {\n",
        "    \"conv_spec\": [[384,384,384],[64]],\n",
        "    \"feature_level\": 2,\n",
        "    \"instance_crop_size\": 96,\n",
        "    \"learned_encoding\": true\n",
        "  }\n",
        "}\n",
        "'''\n",
        "\n",
        "# LacssWithHelper contains both the principal model and the collaborator model\n",
        "model = lacss.modules.lacss.LacssWithHelper(\n",
        "    cfg=json.loads(cfg_json),\n",
        "    aux_edge_cfg={},\n",
        ")\n",
        "\n",
        "trainer = lacss.train.Trainer(\n",
        "    model=model,\n",
        "    losses=[\n",
        "        lacss.losses.LPNLoss(), # detector head loss\n",
        "        lacss.losses.WeaklySupervisedInstanceLoss(), # segmentation head loss\n",
        "        lacss.losses.AuxEdgeLoss(), # consistency loss\n",
        "    ],\n",
        "    optimizer=optax.adam(0.001),\n",
        "    strategy=lacss.train.strategy.VMapped,\n",
        "    seed=1234,\n",
        ")\n",
        "\n",
        "trainer.initialize(train_gen)\n",
        "\n",
        "# use imagenet weights to initialize the convnext backbone\n",
        "imagenet_weights = lacss.modules.convnext.load_weight(\n",
        "    lacss.modules.ConvNeXt(**model.cfg[\"backbone_cfg\"]),\n",
        "    trainer.params['_lacss']['_backbone'], \n",
        "    lacss.modules.convnext.model_urls['convnext_small_1k'], \n",
        "    )\n",
        "\n",
        "params = unfreeze(trainer.params)\n",
        "params['_lacss']['_backbone'] = imagenet_weights\n",
        "\n",
        "trainer.state = trainer.state.replace(\n",
        "    params = freeze(params)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5qcbxs5aomk"
      },
      "source": [
        "## Training\n",
        "\n",
        "Trainer.train() function returns an iterator, stepping through which will drive the training of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09jrmOyjaoAs"
      },
      "outputs": [],
      "source": [
        "n_epoch = 10\n",
        "steps_per_epoch = 3000\n",
        "\n",
        "train_iter = trainer.train(train_gen, rng_cols=[\"droppath\"], training=True)\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  \n",
        "  print(f\"Epoch {epoch+1}\")\n",
        "\n",
        "  for steps in tqdm(range(steps_per_epoch)):\n",
        "\n",
        "      logs = next(train_iter)\n",
        "\n",
        "  print(\", \".join([f\"{k}:{v:.4f}\" for k, v in logs.items()]))\n",
        "\n",
        "  # reset logs\n",
        "  trainer.reset()\n",
        "\n",
        "  # perform validation here\n",
        "\n",
        "  # maybe save a training checkpoint\n",
        "  # trainer.checkpoint(f\"cp-{epoch}\")\n",
        "\n",
        "# save the current model. We only need the principal model\n",
        "trainer.save_model(\"model.pkl\", \"_lacss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_tBseBbc-mw"
      },
      "source": [
        "## Visualize  the model prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qOX43ZnYyX-"
      },
      "outputs": [],
      "source": [
        "image = imageio.imread(join(data_path, 'test', 'img_0001.tif'))\n",
        "gt = imageio.imread(join(data_path, 'test', 'masks_0001.tif'))\n",
        "\n",
        "# normalize\n",
        "img = image - image.mean()\n",
        "img /= img.std()\n",
        "img = img[..., None]\n",
        "\n",
        "# prediction\n",
        "model_output = trainer.model.apply(\n",
        "    dict(params=trainer.params),\n",
        "    image = img,\n",
        ")\n",
        "pred = lacss.ops.patches_to_label(model_output, input_size=img.shape[:2])\n",
        "pred = np.asarray(pred)\n",
        "\n",
        "lacss.utils.show_images([\n",
        "    img,\n",
        "    label2rgb(pred, bg_label=0),\n",
        "    label2rgb(gt, bg_label=0),\n",
        "])\n",
        "titles = ['Input', \"Prediction\", \"Ground Truth\"]\n",
        "[ax.set_title(title) for ax, title in zip(plt.gcf().get_axes(), titles)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-FpYFh3Tl6E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
