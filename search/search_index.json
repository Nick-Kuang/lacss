{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>LACSS is a deep-learning model for single-cell segmentation from microscopy images. </p> <p>References: </p> <ul> <li>https://www.nature.com/articles/s42003-023-04608-5</li> <li>https://arxiv.org/abs/2304.10671</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install --upgrade pip\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\npip install lacss\n</code></pre>"},{"location":"#why-lacss","title":"Why LACSS?","text":"<p>LACSS is designed to utilize point labels for model training. You have three options:</p> Method Data(left) / Label(right) Point Point + Mask Segmentation <p>You can of course also combined these labels in any way you want.</p>"},{"location":"#what-is-included","title":"What is included?","text":"<ul> <li>A library for training LACSS model and performing inference</li> <li>A few pretrained models as transfer learning starting point</li> <li>SMC-based cell tracking utility for people interested in cell tracking</li> </ul>"},{"location":"#how-to-generate-point-label","title":"How to generate point label?","text":"<p>If your data include nuclei counter-stain, the easist way to generate point label for your image is to use a blob detection algorithm on the nuclei images:</p> <p></p>"},{"location":"#give-it-a-try","title":"Give it a try:","text":"<ul> <li>Model training</li> <li>Supervised Training </li> <li>With point label + mask </li> <li> <p>With point label only </p> </li> <li> <p>Inference </p> </li> </ul>"},{"location":"api/data/","title":"lacss.data","text":"<p>Lacss Data Pipeline API</p>"},{"location":"api/data/#tf-dataset","title":"TF Dataset","text":""},{"location":"api/data/#lacss.data.generator.dataset_from_coco_annotations","title":"<code>dataset_from_coco_annotations(annotation_file, image_path, image_shape=[None, None, 3], mask_shape=[48, 48])</code>","text":"<p>Obtaining a tensowflow dataset from coco annotations. See coco_generator_full()</p> <p>Parameters:</p> Name Type Description Default <code>annotation_file</code> <code>str</code> <p>Path to coco annotation files</p> required <code>image_path</code> <code>str</code> <p>Path to image directory</p> required <code>image_shape</code> <code>tuple</code> <p>The expect image shapes. Use None to represent variable dimensions.</p> <code>[None, None, 3]</code> <code>mask_shape</code> <code>tuple</code> <p>If supplied, all the instance segmentations will be croped and resized to the specifed size. Otherwise, the segmentations are uncropped (in original image size)</p> <code>[48, 48]</code> <p>Returns:</p> Type Description <code>tf.Dataset</code> <p>A tensorflow dataset.</p>"},{"location":"api/data/#lacss.data.generator.dataset_from_simple_annotations","title":"<code>dataset_from_simple_annotations(annotation_file, image_path, image_shape=[None, None, 3], **kwargs)</code>","text":"<p>Obtaining a tensowflow dataset from simple annotatiion. See simple_generator()</p> <p>Parameters:</p> Name Type Description Default <code>annotation_file</code> <code>str</code> <p>Path to the json format annotation file.</p> required <code>image_path</code> <code>str</code> <p>Path to the image directory.</p> required <code>image_shape</code> <p>The expect image shapes. Use None to represent variable dimensions.</p> <code>[None, None, 3]</code> <p>Returns:</p> Type Description <code>tf.Dataset</code> <p>A tensorflow dataset object</p>"},{"location":"api/data/#lacss.data.generator.dataset_from_img_mask_pairs","title":"<code>dataset_from_img_mask_pairs(imgfiles, maskfiles, image_shape=[None, None, 3], **kwargs)</code>","text":"<p>Obtaining a tensowflow dataset from image/label pairs.         See img_mask_pair_generator()</p> <p>Parameters:</p> Name Type Description Default <code>imgfiles</code> <code>Sequence[str]</code> <p>List of file pathes to input image file.</p> required <code>maskfiles</code> <code>Sequence[str]</code> <p>List of file pathes to label image file.</p> required <code>image_shape</code> <p>The expect image shapes. Use None to represent variable dimensions.</p> <code>[None, None, 3]</code> <p>Returns:</p> Type Description <code>tf.Dataset</code> <p>A tensorflow dataset object</p>"},{"location":"api/data/#data-augmentation","title":"Data augmentation","text":"<p>The augmentation functions for TF Dataset inputs.</p>"},{"location":"api/data/#lacss.data.augment.crop_to_roi","title":"<code>crop_to_roi(inputs, *, roi, area_ratio_threshold=1.0, p=1.0)</code>","text":"<p>Crop image to bounding-box ROI</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> <li>masks</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>roi</code> <code>tuple[int, int, int, int]</code> <p>Rectangle roi in yxyx format</p> <code>area_ratio_threshold</code> <code>float</code> <p>remove instances if the bbox's relative remaining area is below this threshold</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.flip_left_right","title":"<code>flip_left_right(inputs, *, p=1.0)</code>","text":"<p>Flip image left-right</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.flip_up_down","title":"<code>flip_up_down(inputs, *, p=1.0)</code>","text":"<p>Flip image up-down</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.pad","title":"<code>pad(inputs, *, paddings, constant_values=0, p=1.0)</code>","text":"<p>Pad image and labels.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>paddings</code> <code>int | tuple[int, int]</code> <p>either a tuple or a single value. If latter, use the same padding for both x and y axis</p> <code>constant_values</code> <code>float</code> <p>the value to fill the padded area</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.pad_to_size","title":"<code>pad_to_size(inputs, *, target_size, constant_values=0, p=1.0)</code>","text":"<p>Pad image and labels to a target size. Padding is applied so that the orginal scene is centered in the output.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>target_size</code> <code>tuple[int, int]</code> <p>target image size</p> <code>constant_values</code> <code>float</code> <p>the value to fill the padded area</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.random_crop","title":"<code>random_crop(inputs, *, target_size, area_ratio_threshold=1.0, p=1.0)</code>","text":"<p>Random crop to a set target size</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> <li>masks</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>target_size</code> <code>tuple[int, int]</code> <p>the target size</p> <code>area_ratio_threshold</code> <code>float</code> <p>remove instances if the bbox's relative remaining area is below this threshold</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.random_crop_or_pad","title":"<code>random_crop_or_pad(inputs, *, target_size, constant_values=0, area_ratio_threshold=1.0, p=1.0)</code>","text":"<p>Random crop or pad image to specified target_size.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> <li>bboxes</li> <li>masks</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>target_size</code> <code>tuple[int, int]</code> <p>target size</p> <code>constant_values</code> <code>float</code> <p>the value to fill the padded area</p> <code>area_ratio_threshold</code> <code>float</code> <p>remove instances if the bbox's relative remaining area is below this threshold</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.random_resize","title":"<code>random_resize(inputs, *, scaling, keep_aspect_ratio=False, p=1.0)</code>","text":"<p>Resize image by a random amount</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>scaling</code> <code>float | tuple[float, float]</code> <p>range of scale, e.g, [0.8, 1.5]. If a single value (s), the range is [1-s, 1+s]</p> <code>keep_aspect_ratio</code> <code>bool</code> <p>Whether to scale x/y the same amount.</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#lacss.data.augment.resize","title":"<code>resize(inputs, *, target_size, p=1.0)</code>","text":"<p>Resize image and labels</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>data from TF dataset. Affected elements are:</p> <ul> <li>image</li> <li>image_mask</li> <li>centroids</li> </ul> required <p>Other Parameters:</p> Name Type Description <code>target_size</code> <code>tuple[int, int]</code> <p>target size</p> <code>p</code> <code>float</code> <p>probability of applying transformation</p>"},{"location":"api/data/#python-generator","title":"Python generator","text":""},{"location":"api/data/#lacss.data.generator.coco_generator_full","title":"<code>coco_generator_full(annotation_file, image_path, mask_shape=None)</code>","text":"<p>A generator function to produce coco-annotated data</p> <p>Parameters:</p> Name Type Description Default <code>annotation_file</code> <code>str</code> <p>Path to coco annotation files</p> required <code>image_path</code> <code>str</code> <p>Path to image directory</p> required <code>mask_shape</code> <code>Optional[tp.Tuple[int, int]]</code> <p>If supplied, all the instance segmentations will be croped and resized to the specifed size. Otherwise, the segmentations are uncropped (in original image size)</p> <code>None</code> <p>Yields:</p> Type Description <code>dict</code> <p>A data dictionary with thse keys:</p> <ul> <li>id: data id</li> <li>filename: image filename</li> <li>image: an array [H, W, C]</li> <li>masks: segmentation masks. [N, H, W] or [N,] + mask_shape</li> <li>centroids: yx format.</li> <li>bboxes: y0x0y1x1 format.</li> <li>label: an array [H, W] representing pixel labels of all instances.</li> </ul>"},{"location":"api/data/#lacss.data.generator.simple_generator","title":"<code>simple_generator(annotation_file, image_path)</code>","text":"<p>A simple generator function to produce image data labeled with points and image-level segmentaion.</p> <p>Parameters:</p> Name Type Description Default <code>annotation_file</code> <code>str</code> <p>Path to the json format annotation file.</p> required <code>image_path</code> <code>str</code> <p>Path to the image directory.</p> required <p>Yields:</p> Type Description <code>dict</code> <p>A data dictionary with thse keys:</p> <ul> <li>img_id: data id</li> <li>image: an array [H, W, C]</li> <li>image_mask: segmentation masks for the image. [H, W]</li> <li>centroids: yx format.</li> </ul>"},{"location":"api/data/#lacss.data.generator.img_mask_pair_generator","title":"<code>img_mask_pair_generator(ds_files)</code>","text":"<p>A generator function to produce image data labeled with segmentation labels.     In this case, one has paired input images and label images as files on disk.</p> <p>Parameters:</p> Name Type Description Default <code>ds_file</code> <p>A tuple of (image_list, label_list). The image_list are pathes to images. The label_list are pathes to labels.</p> required <p>Yields:</p> Type Description <code>dict</code> <p>A data dictionary with thse keys:</p> <ul> <li>img_id: data id</li> <li>image: an array [H, W, C]</li> <li>centroids: yx format.</li> <li>bboxes: y0x0y1x1 format.</li> <li>label: an array [H, W] representing pixel labels of all instances.</li> </ul>"},{"location":"api/deploy/","title":"lacss.deploy","text":"<p>Attributes:</p> Name Type Description <code>model_urls</code> <code>Mapping[str, str]</code> <p>URLs for build-in pretrain models. e.g model_urls[\"livecell\"].</p>"},{"location":"api/deploy/#lacss.deploy.Predictor","title":"<code>Predictor</code>","text":"<p>Main class interface for model deployment. This is the only class you need if you don't train your own model</p> <p>Examples:</p> <p>The most common use case is to use a build-in pretrained model.</p> <pre><code>import lacss.deploy\n\n# look up the url of a build-in mode\nurl = lacss.deploy.model_urls[\"livecell\"]\n\n# create the predictor instance\npredictor = lacss.deploy.Predictor(url)\n\n# make a prediction\nlabel = predictor.predict_label(image)\n</code></pre> <p>Attributes:</p> Name Type Description <code>module</code> <code>nn.Module</code> <p>The underlying FLAX module</p> <code>params</code> <code>Params</code> <p>Model weights.</p> <code>detector</code> <code>lacss.modules.Detector</code> <p>The detector submodule for convinence. A common use case is to customize this submodule during inference. e.g.</p> <pre><code>predictor.detector.test_min_score = 0.5\n</code></pre> <p>Detector submodule has no trained paramters.</p>"},{"location":"api/deploy/#lacss.deploy.Predictor.__call__","title":"<code>__call__(inputs, **kwargs)</code>","text":"<p>Do inference.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Model inputs.</p> required <p>Returns:</p> Type Description <p>A dict of LACSS model outputs.</p>"},{"location":"api/deploy/#lacss.deploy.Predictor.__init__","title":"<code>__init__(url, precompile_shape=None)</code>","text":"<p>Construct Predictor</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>A URL or local path to the saved model. URLs for build-in pretrained models can be found in lacss.deploy.model_urls</p> required <code>precompile_shape</code> <code>Optional[Shape]</code> <p>Image shape(s) for precompiling. Otherwise the model will be recompiled for every new input image shape.</p> <code>None</code>"},{"location":"api/deploy/#lacss.deploy.Predictor.pickle","title":"<code>pickle(save_path)</code>","text":"<p>Save the model by pickling.     In the form of (model_config:dict, weights:FrozenDict).</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <p>Path to the pkl file</p> required"},{"location":"api/deploy/#lacss.deploy.Predictor.predict","title":"<code>predict(image, *, min_area=0, remove_out_of_bound=False, scaling=1.0, return_label=False, **kwargs)</code>","text":"<p>Predict segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>A ndarray of (h,w,c) format. Value of c must be 1-3</p> required <p>Other Parameters:</p> Name Type Description <code>min_area</code> <code>float</code> <p>Minimum area of a valid prediction.</p> <code>remove_out_of_bound</code> <code>bool</code> <p>Whether to remove out-of-bound predictions. Default is False.</p> <code>scaling</code> <code>float</code> <p>A image scaling factor. If not 1, the input image will be resized internally before fed to the model. The results will be resized back to the scale of the orginal input image.</p> <code>return_label</code> <code>bool</code> <p>Whether to output full model prediction or segmentation label.</p> <p>Returns:</p> Type Description <code>Union[dict, Array]</code> <p>If return_label is False, returns model predictions with following elements:</p> <ul> <li>pred_scores: The prediction scores of each instance.</li> <li>pred_bboxes: The bounding-boxes of detected instances in y0x0y1x1 format</li> <li>instance_output: A 3d array representing segmentation instances.</li> <li>instance_yc: The meshgrid y-coordinates of the instances.</li> <li>instance_xc: The meshgrid x-coordinates of the instances.</li> <li>instance_mask: a masking tensor. Invalid instances are maked with 0.</li> </ul> <code>Union[dict, Array]</code> <p>otherwise return a segmentation label.</p>"},{"location":"api/deploy/#lacss.deploy.Predictor.predict_label","title":"<code>predict_label(image, *, score_threshold=0.5, **kwargs)</code>","text":"<p>Predict segmentation in image label format</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>Input image.</p> required <code>score_threshold</code> <code>float</code> <p>The minimal prediction scores.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Array</code> <p>A [H, W] array. The values indicate the id of the cells. For cells with overlapping areas, the pixel is assigned for the cell with higher prediction scores.</p>"},{"location":"api/deploy/#lacss.deploy.Predictor.predict_on_large_image","title":"<code>predict_on_large_image(image, gs, ss, *, scaling=1, min_area=0, nms_iou=0.3, segmentation_threshold=0.5, score_threshold=0.5, return_label=False, **kwargs)</code>","text":"<p>Make prediction on very large image by dividing into a grid.</p> <p>Direct model prediction on large image may cause out-of-memory error. This method divided the large image to smaller grid and then stitch the results to form the complete prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>An image with (H, W, C) format.</p> required <code>gs</code> <code>int</code> <p>An int value. Grid size of the computation.</p> required <code>ss</code> <code>int</code> <p>An int value of stepping size. Must be small than gs to produce valid results.</p> required <p>Other Parameters:</p> Name Type Description <code>scaling</code> <code>float</code> <p>A image scaling factor.</p> <code>nms_iou</code> <code>float</code> <p>Optional iou threshold for the non-max-suppression post-processing. Default is 0, which disable non-max-suppression.</p> <code>min_area</code> <code>int</code> <p>Optional minimum area for the instance to be included in the results. Default is 0.</p> <code>segmentation_threshold</code> <code>float</code> <p>Default is 0.5.</p> <code>return_label</code> <code>bool</code> <p>Whether to output full model prediction or segmentation label.</p> <p>Returns:</p> Type Description <code>Union[dict, ArrayLike]</code> <p>Segmention label if output_label is True, otherwise a dict of full model predictions.</p>"},{"location":"api/deploy/#lacss.deploy.load_from_pretrained","title":"<code>load_from_pretrained(pretrained)</code>","text":"<p>Load a saved model.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained</code> <p>The url to the saved model.</p> required"},{"location":"api/losses/","title":"lacss.losses","text":""},{"location":"api/losses/#lacss.losses.instance.self_supervised_instance_loss","title":"<code>self_supervised_instance_loss(preds, *, soft_label=True, **kwargs)</code>","text":"<p>Unsupervised instance loss</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <p>Model prediction dict.</p> required <code>soft_label</code> <code>bool</code> <p>If False, it convertes image mask prediction to hard label (ie. 0 or 1), before computing loss.</p> <code>True</code>"},{"location":"api/losses/#lacss.losses.instance.supervised_instance_loss","title":"<code>supervised_instance_loss(preds, labels, **kwargs)</code>","text":"<p>LACSS instance loss, supervised with segmentation label</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <p>Model predictions</p> required <code>labels</code> <p>Label dictionary. Two types of segmentation labels are accepted. If <code>labels[\"gt_labels\"]</code> is provided, its value is treated as pixel labels of the image. Otherwisde, one much supply both <code>labels[\"gt_bboxes\"]</code> and <code>labels[\"gt_masks\"]</code>. The gt_masks is a 3D array representing all segmentation resized to a fixed dimension.</p> required"},{"location":"api/losses/#lacss.losses.instance.weakly_supervised_instance_loss","title":"<code>weakly_supervised_instance_loss(preds, labels, inputs, *, ignore_mask=False, **kwargs)</code>","text":"<p>Instance loss supervised by image mask instead of instance masks</p>"},{"location":"api/losses/#lacss.losses.detection.detection_loss","title":"<code>detection_loss(preds, gamma=2.0, **kwargs)</code>","text":"<p>LPN detection loss</p>"},{"location":"api/losses/#lacss.losses.detection.localization_loss","title":"<code>localization_loss(preds, delta=1.0, **kwargs)</code>","text":"<p>LPN localization loss</p>"},{"location":"api/losses/#lacss.losses.detection.lpn_loss","title":"<code>lpn_loss(preds, gamma=2.0, w1=1.0, w2=1.0, **kwargs)</code>","text":"<p>LPN loss</p>"},{"location":"api/losses/#lacss.losses.auxiliary.aux_size_loss","title":"<code>aux_size_loss(preds, inputs, *, weight=0.01, **kwargs)</code>","text":"<p>Auxillary loss to prevent model collapse</p>"},{"location":"api/losses/#lacss.losses.auxiliary.self_supervised_edge_loss","title":"<code>self_supervised_edge_loss(preds, inputs, **kwargs)</code>","text":"<p>Cell border prediction consistency loss</p>"},{"location":"api/losses/#lacss.losses.auxiliary.self_supervised_segmentation_loss","title":"<code>self_supervised_segmentation_loss(preds, inputs, *, offset_sigma=jnp.array([10.0]), offset_scale=jnp.array([2.0]), **kwargs)</code>","text":"<p>Image segmentation consistenct loss for the collaboraor model</p>"},{"location":"api/modules/","title":"lacss.modules","text":""},{"location":"api/modules/#lacss.modules.Lacss","title":"<code>lacss.modules.Lacss</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Main class for LACSS model</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>ConvNeXt</code> <p>The ConvNeXt backbone</p> <code>lpn</code> <code>LPN</code> <p>The LPN head for detecting cell location</p> <code>detector</code> <code>Detector</code> <p>A weight-less module interpreting lpn output</p> <code>segmentor</code> <code>Segmentor</code> <p>The segmentation head</p>"},{"location":"api/modules/#lacss.modules.lacss.Lacss.__call__","title":"<code>__call__(image, gt_locations=None, *, training=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image</code> <code>jnp.ndarray</code> <p>[H, W, C]</p> required <code>gt_locations</code> <code>jnp.ndarray</code> <p>[M, 2] if training, otherwise None</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>a dict of model outputs</p>"},{"location":"api/modules/#lacss.modules.lacss.Lacss.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Factory method to build an Lacss instance from a configuration dictionary</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A configuration dictionary.</p> required <p>Returns:</p> Type Description <p>An Lacss instance.</p>"},{"location":"api/modules/#lacss.modules.lacss.Lacss.get_config","title":"<code>get_config()</code>","text":"<p>Convert to a configuration dict. Can be serialized with json</p> <p>Returns:</p> Name Type Description <code>config</code> <code>dict</code> <p>a configuration dict</p>"},{"location":"api/modules/#lacss.modules.ConvNeXt","title":"<code>lacss.modules.ConvNeXt</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>ConvNeXt CNN backbone</p> <p>Attributes:</p> Name Type Description <code>patch_size</code> <code>int</code> <p>Stem patch size</p> <code>depths</code> <code>Sequence[int]</code> <p>Number of blocks at each stage.</p> <code>dims</code> <code>Sequence[int]</code> <p>Feature dimension at each stage.</p> <code>drop_path_rate</code> <code>float</code> <p>Stochastic depth rate.</p> <code>layer_scale_init_value</code> <code>float</code> <p>Init value for Layer Scale.</p> <code>out_channels</code> <code>int</code> <p>FPN output channels. Setting this to -1 disable the FPN, in which case the model output only encoder outputs.</p>"},{"location":"api/modules/#lacss.modules.convnext.ConvNeXt.__call__","title":"<code>__call__(x, *, training=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Image input.</p> required <code>training</code> <code>Optional[bool]</code> <p>Whether run the network in training mode (i.e. with stochastic depth)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataDict, DataDict]</code> <p>A tuple of (encoder_outputs, decoder_outputs). Both are dictionaries mapping feature scale (e.g. \"2\") to features. If out_channels is -1, the decoder_output is None.</p>"},{"location":"api/modules/#lacss.modules.convnext.ConvNeXt.get_imagenet_weights","title":"<code>get_imagenet_weights(model_type)</code>","text":"<p>Get imagenet weights</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The expected model specification. This must match the current instance attributes.</p> <ul> <li>tiny: dims=(96, 192, 384, 768), depths=(3,3,9,3)</li> <li>small: dims=(96, 192, 384, 768), depths=(3,3,27,3)</li> <li>base: dims=(128, 256, 512, 1024), depths=(3,3,27,3)</li> <li>large: dims=(192, 384, 768, 1536), depths=(3,3,27,3)</li> <li>X-large: dims=(256, 512, 1024, 2048), depths=(3,3,27,3)</li> </ul> required <p>Returns:</p> Type Description <code>Params</code> <p>A frozen dict representing weights of the current module</p>"},{"location":"api/modules/#lacss.modules.LPN","title":"<code>lacss.modules.LPN</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Location detection head</p> <p>Attributes:</p> Name Type Description <code>feature_levels</code> <code>Sequence[int]</code> <p>Input feature level, e.g. [2, 3, 4]</p> <code>conv_spec</code> <code>Tuple[Sequence[int], Sequence[int]]</code> <p>Conv layer specification</p> <code>detection_roi</code> <code>float</code> <p>Parameter for label smoothing</p>"},{"location":"api/modules/#lacss.modules.lpn.LPN.__call__","title":"<code>__call__(inputs, scaled_gt_locations=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Mapping[str, ArrayLike]</code> <p>feature dict: {'lvl': [H, W, C]}</p> required <code>scaled_gt_locations</code> <code>Optional[ArrayLike]</code> <p>scaled 0..1 [N, 2], only valid in training</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict of features</p> <ul> <li>lpn_scores: dict: {'lvl': [H, W, 1]}</li> <li>lpn_regressions: dict {'lvl': [H, W, 2]}</li> <li>gt_lpn_scores: dict {'lvl': [H, W, 1]}, only if training</li> <li>gt_lpn_regressions: dict {'lvl': [H, W, 2]}, only if training</li> </ul>"},{"location":"api/modules/#lacss.modules.Detector","title":"<code>lacss.modules.Detector</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>A weightless module that conver LPN output to a list of locations. Non-max-supression is     appplied to remove redundant detections.</p> <p>Attributes:</p> Name Type Description <code>train_nms_threshold</code> <code>float</code> <p>Threshold for non-max-suppression during training</p> <code>train_pre_nms_topk</code> <code>int</code> <p>If &gt; 0, only the top_k scored locations will be analyzed during training</p> <code>train_max_output</code> <code>int</code> <p>Maximum number of outputs during training</p> <code>train_min_score</code> <code>float</code> <p>Mininum scores to be considered during training</p> <code>max_proposal_offset</code> <code>float</code> <p>During training, if a detected location is with in this distance threshold of a ground-truth location, replacing the ground truth location with the detection one for segmentation purpose.</p> <code>test_nms_threshold</code> <code>float</code> <p>Threshold for non-max-suppression during testing</p> <code>test_pre_nms_topk</code> <code>int</code> <p>If &gt; 0, only the top_k scored locations will be analyzed during testing</p> <code>test_max_output</code> <code>int</code> <p>Maximum number of outputs during testing</p> <code>test_min_score</code> <code>float</code> <p>Mininum scores to be considered during testing</p>"},{"location":"api/modules/#lacss.modules.detector.Detector.__call__","title":"<code>__call__(scores, regressions, gt_locations=None, *, training=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Mapping[str, ArrayLike]</code> <p>{'scale', [H, W, 1]) output from LPN</p> required <code>regressions</code> <code>Mapping[str, ArrayLike]</code> <p>{'scale': [H, W, 2]} output from LPN</p> required <code>gt_locations</code> <code>Optional[ArrayLike]</code> <p>Ground-truth call locations. This can be an array  of [N, 2] (training) or None (inference). Maybe padded with -1</p> <code>None</code> <code>training</code> <code>Optional[bool]</code> <p>Whether to run the module in training mode or not</p> <code>None</code> <p>Returns:</p> Type Description <code>DataDict</code> <p>a dictionary of values.</p> <ul> <li>pred_locations: Sorted array based on scores</li> <li>pred_scores: Sorted array, padded with -1</li> <li>training_locations: This value exists during training only.</li> </ul>"},{"location":"api/modules/#lacss.modules.Segmentor","title":"<code>lacss.modules.Segmentor</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>LACSS segmentation head.</p> <p>Attributes:</p> Name Type Description <code>feature_level</code> <code>int</code> <p>The scale of the feature used for segmentation</p> <code>conv_spec</code> <code>Tuple[Sequence[int], Sequence[int]]</code> <p>conv_block definition, e.g. ((384,384,384), (64,))</p> <code>instance_crop_size</code> <code>int</code> <p>Crop size for segmentation.</p> <code>with_attention</code> <code>int</code> <p>Whether use spatial attention layer.</p> <code>learned_encoding</code> <code>bool</code> <p>Whether to use hard-coded position encoding.</p> <code>encoder_dims</code> <code>Sequence[int]</code> <p>Dim of the position encoder, if using learned encoding. Default is (8,8,4)</p>"},{"location":"api/modules/#lacss.modules.segmentor.Segmentor.__call__","title":"<code>__call__(features, locations)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>features</code> <code>Mapping[str, ArrayLike]</code> <p>{'scale' [H, W, C]} feature dictionary from the backbone.</p> required <code>locations</code> <code>ArrayLike</code> <p>[N, 2]  normalized to image size.</p> required <p>Returns:</p> Type Description <code>DataDict</code> <p>A dictionary of values representing segmentation outputs.</p> <ul> <li>instance_output: [N, crop_size, crop_size]</li> <li>instance_mask; [N, 1, 1] boolean mask indicating valid outputs</li> <li>instance_yc: [N, crop_size, crop_size] meshgrid y coordinates</li> <li>instance_xc: [N, crop_size, crop_size] meshgrid x coordinates</li> </ul>"},{"location":"api/ops/","title":"lacss.ops","text":"<p>Ops on bounding-boxes</p> <p>All functions here are degisned to work as either a numpy op or a jax op depending on the data type of the input.</p> <p>Various functions deals with segmentation pathces</p> <p>All functions here takes unbatched input. Use vmap to convert to batched data</p>"},{"location":"api/ops/#lacss.ops.boxes.box_area","title":"<code>box_area(box)</code>","text":"<p>Computes area of boxes.</p> <p>Parameters:</p> Name Type Description Default <code>box</code> <code>ArrayLike</code> <p>a float Tensor with [..., N, 4].</p> required <p>Returns:</p> Type Description <code>ArrayLike</code> <p>a float Tensor with [..., N]</p>"},{"location":"api/ops/#lacss.ops.boxes.box_intersection","title":"<code>box_intersection(gt_boxes, boxes)</code>","text":"<p>Compute pairwise intersection areas between boxes.</p> <p>Parameters:</p> Name Type Description Default <code>gt_boxes</code> <code>ArrayLike</code> <p>[..., N, 4]</p> required <code>boxes</code> <code>ArrayLike</code> <p>[..., M, 4]</p> required <p>Returns:</p> Type Description <code>ArrayLike</code> <p>a float Tensor with shape [..., N, M] representing pairwise intersections.</p>"},{"location":"api/ops/#lacss.ops.boxes.box_iou_similarity","title":"<code>box_iou_similarity(gt_boxes, boxes)</code>","text":"<p>Computes pairwise intersection-over-union between box collections.</p> <p>Parameters:</p> Name Type Description Default <code>gt_boxes</code> <code>ArrayLike</code> <p>a float Tensor with [..., N, 4].</p> required <code>boxes</code> <code>ArrayLike</code> <p>a float Tensor with [..., M, 4].</p> required <p>Returns:</p> Type Description <code>ArrayLike</code> <p>a Tensor with shape [..., N, M] representing pairwise iou scores.</p>"},{"location":"api/ops/#lacss.ops.image.sorbel_edges","title":"<code>sorbel_edges(image)</code>","text":"<p>Returns a tensor holding Sobel edge maps.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = random.uniform(key, shape=[3, 28, 28])\n&gt;&gt;&gt; sobel = sobel_edges(image)\n&gt;&gt;&gt; sobel_y = sobel[0, :, :, :] # sobel in y-direction\n&gt;&gt;&gt; sobel_x = sobel[1, :, :, :] # sobel in x-direction\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ArrayLike</code> <p>[n, h, w]</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Tensor holding edge maps for each channel. [2, n, h, w]</p>"},{"location":"api/ops/#lacss.ops.image.sub_pixel_crop_and_resize","title":"<code>sub_pixel_crop_and_resize(img, bbox, output_shape, out_of_bound_value=0)</code>","text":"<p>Retrieve image values of a bbox resize output. Used for ROI-Align</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ArrayLike</code> <p>Array of shape [H, W, ...]</p> required <code>bbox</code> <code>ArrayLike</code> <p>[y0, x0, y1, x1]</p> required <code>output_shape</code> <code>Shape</code> <p>[h, w]</p> required <code>out_of_bound_value</code> <code>float</code> <p>optional float constant, defualt 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>values</code> <code>Array</code> <p>[h, w, ...], float</p>"},{"location":"api/ops/#lacss.ops.image.sub_pixel_samples","title":"<code>sub_pixel_samples(img, locs, out_of_bound_value=0, edge_indexing=False)</code>","text":"<p>Retrieve image values as non-integer locations by interpolation</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ArrayLike</code> <p>Array of shape [D1,D2,..,Dk, ...]</p> required <code>locs</code> <code>ArrayLike</code> <p>Array of shape [d1,d2,..,dn, k]</p> required <code>out_of_bound_value</code> <code>float</code> <p>optional float constant, defualt 0.</p> <code>0</code> <code>edge_indexing</code> <code>bool</code> <p>if True, the index for the first value in img is 0.5, otherwise 0. Default is False</p> <code>False</code> <p>Returns:</p> Name Type Description <code>values</code> <code>Array</code> <p>[d1,d2,..,dn, ...], float</p>"},{"location":"api/ops/#lacss.ops.locations.distance_similarity","title":"<code>distance_similarity(pred_locations, gt_locations)</code>","text":"<p>Compute distance similarity matrix pairwise similarity = 1 / distance ^2</p> <p>Parameters:</p> Name Type Description Default <code>pred_locations</code> <code>ArrayLike</code> <p>[N, 2] use -1 to mask out invalid locations</p> required <code>gt_locations</code> <code>ArrayLike</code> <p>[K, 2] use -1 to mask out invalid locations</p> required <p>Returns:</p> Name Type Description <code>similarity_matrix</code> <code>Array</code> <p>[N, k]</p>"},{"location":"api/ops/#lacss.ops.locations.location_matching","title":"<code>location_matching(pred_locations, gt_locations, threshold)</code>","text":"<p>Match predicted location to gt locations</p> <p>Parameters:</p> Name Type Description Default <code>pred_locations</code> <code>ArrayLike</code> <p>r [N, 2]</p> required <code>gt_locations</code> <code>ArrayLike</code> <p>[K, 2]</p> required <code>threshold</code> <code>float</code> <p>float. Maximum distance to be matched</p> required <p>Returns:</p> Name Type Description <code>matches</code> <code>Array</code> <p>[N], indices of the matches location in gt list</p> <code>indicators</code> <code>Array</code> <p>[N] bool</p>"},{"location":"api/ops/#lacss.ops.locations.locations_to_labels","title":"<code>locations_to_labels(locations, target_shape, threshold=1.5)</code>","text":"<p>Generate labels as LPN regression targets</p> <p>Parameters:</p> Name Type Description Default <code>locations</code> <code>ArrayLike</code> <p>[N, 2] float32 true location values. scaled 0..1, masking out invalid with -1</p> required <code>target_shape</code> <code>Shape</code> <p>(H, W)  int</p> required <code>threshold</code> <code>float</code> <p>distance threshold for postive label</p> <code>1.5</code> <p>Returns:</p> Name Type Description <code>score_target</code> <code>Array</code> <p>[H, W, 1] int32</p> <code>regression_target</code> <code>Array</code> <p>[H, W, 2] float tensor</p>"},{"location":"api/ops/#lacss.ops.nms.sorted_non_max_suppression","title":"<code>sorted_non_max_suppression(scores, boxes, max_output_size, threshold=0.5, min_score=0, return_selection=False)</code>","text":"<p>non-maximum suppression for either bboxes or points.</p> Assumption <ul> <li>The boxes are sorted by scores</li> </ul> <p>The overal design of the algorithm is to handle boxes tile-by-tile:</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ArrayLike</code> <p>[N]</p> required <code>boxes</code> <code>ArrayLike</code> <p>[N, C]  C=4 for boxes, C=2 for locations</p> required <code>max_output_size</code> <code>int</code> <p>a positive scalar integer</p> required <code>threshold</code> <code>float</code> <p>a scalar float, can be negative</p> <code>0.5</code> <code>min_score</code> <code>float</code> <p>min score to be selected, default 0</p> <code>0</code> <code>return_selection</code> <code>bool</code> <p>whether also return the boolean indicator</p> <code>False</code> <p>Returns:</p> Name Type Description <code>nms_scores</code> <code>Sequence[Array]</code> <p>[M].  M = max_output_size</p> <code>nms_proposals</code> <code>Sequence[Array]</code> <p>[M, C].</p> <code>selection</code> <code>Sequence[Array]</code> <p>[N] a boolean indicator of selection status of original input</p>"},{"location":"api/ops/#lacss.ops.patches.bboxes_of_patches","title":"<code>bboxes_of_patches(pred, threshold=0.5)</code>","text":"<p>Compute the instance bboxes from model predictions</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Union[Sequence, DataDict]</code> <p>A model prediction dictionary:</p> required <code>threshold</code> <code>float</code> <p>for segmentation, default 0.5</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>bboxes</code> <code>jnp.ndarray</code> <p>[n, 4] bboox for empty patches are filled with -1</p>"},{"location":"api/ops/#lacss.ops.patches.gather_patches","title":"<code>gather_patches(features, locations, patch_size)</code>","text":"<p>extract feature patches according to a list of locations</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ArrayLike</code> <p>[H,W,C] standard 2D feature map</p> required <code>locations</code> <code>ArrayLike</code> <p>[N, 2] float32, scaled 0..1</p> required <code>patch_size</code> <code>int</code> <p>int</p> required <p>Returns:</p> Name Type Description <code>patches</code> <code>Array</code> <p>[N, patch_size, patch_size, C]</p> <code>y0</code> <code>Array</code> <p>[N]: y0 coordinates of patches</p> <code>x0</code> <code>Array</code> <p>[N]: x0 coordinates of patches</p>"},{"location":"api/ops/#lacss.ops.patches.indices_of_patches","title":"<code>indices_of_patches(pred, input_size=None, threshold=0.5)</code>","text":"<p>Compute yx coodinates of all segmented instances</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Union[Sequence, DataDict]</code> <p>A model prediction dictionary</p> required <code>threshold</code> <code>float</code> <p>float</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Array</code> <p>[N, 3] array. The first two columns are y-x coordinates. The third column is a index value for different instances.</p>"},{"location":"api/ops/#lacss.ops.patches.iou_patches_and_labels","title":"<code>iou_patches_and_labels(pred, labels, BLOCK_SIZE=128, threshold=0.5)</code>","text":"<p>Compute iou between prediction and ground truth label.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>DataDict</code> <p>A model prediction dictionary</p> required <code>labels</code> <code>ArrayLike</code> <p>image label. bg_label = 0</p> required <code>threshold</code> <code>float</code> <p>for segmentation. default is 0.5</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Array</code> <p>[n, m] array of iou values.</p>"},{"location":"api/ops/#lacss.ops.patches.ious_of_patches_from_same_image","title":"<code>ious_of_patches_from_same_image(pred, threshold=0.5)</code>","text":"<p>Compute IOUs among instances from the same image. Most likely used for nms.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>DataDict</code> <p>A model prediction dictionary</p> required <code>threshold</code> <code>float</code> <p>Segmentation threshold. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ious</code> <code>Array</code> <p>IOUs as an upper triagle matrix.</p>"},{"location":"api/ops/#lacss.ops.patches.non_max_suppress_predictions","title":"<code>non_max_suppress_predictions(pred, iou_threshold=0.6)</code>","text":"<p>Perform nms on the model prediction based on mask IOU</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>DataDict</code> <p>A model prediction dictionary</p> required <code>ious_threshold</code> <p>default 0.6</p> required <p>Returns:</p> Name Type Description <code>mask</code> <code>Array</code> <p>boolean mask of cells not supressed</p>"},{"location":"api/ops/#lacss.ops.patches.patches_to_label","title":"<code>patches_to_label(pred, input_size, mask=None, score_threshold=0.5, threshold=0.5, min_cell_area=0)</code>","text":"<p>convert patch output to the image label</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>DataDict</code> <p>A model prediction dictionary</p> required <code>input_size</code> <code>Shape</code> <p>shape of the input image. Tuple of H, W</p> required <code>mask</code> <code>Optional[ArrayLike]</code> <p>boolean indicators masking out unwanted instances. Default is None (all cells)</p> <code>None</code> <code>score_threshold</code> <code>float</code> <p>otional, min_score to be included. Default is .5.</p> <code>0.5</code> <code>threshold</code> <code>float</code> <p>segmentation threshold.  Default .5</p> <code>0.5</code> <code>min_cell_area</code> <code>int</code> <p>optional minimal cell area to be plotted, default 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>label</code> <code>Array</code> <p>[height, width]</p>"},{"location":"api/ops/#lacss.ops.patches.patches_to_segmentations","title":"<code>patches_to_segmentations(pred, input_size, threshold=0.5)</code>","text":"<p>Expand the predicted patches to the full image size. The default model segmentation output shows only a small patch around each instance. This function expand each patch to the size of the orginal image.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>DataDict</code> <p>A model prediction dictionary</p> required <code>input_size</code> <code>Shape</code> <p>shape of the input image. Tuple of H, W</p> required <code>threshold</code> <code>float</code> <p>for segmentation. Default is 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>segmentations</code> <code>Array</code> <p>[n, height, width] n full0-size segmenatation masks.</p>"},{"location":"api/ops/#lacss.ops.patches.rescale_patches","title":"<code>rescale_patches(pred, scale)</code>","text":"<p>Rescale/resize instance outputs in a sub-pixel accurate way. If the input image was rescaled, this function take care of rescaling the predictions to the orginal coodinates.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>DataDict</code> <p>A model prediction dictionary</p> required <code>scale</code> <code>float</code> <p>The scaling value. The function does not take a noop shortcut even if scale is 1.</p> required <p>A tuple of three arrays</p> Name Type Description <code>patches</code> <code>Array</code> <p>a 3D array of the rescaled segmentation patches. The array shape should be different from the orignal patches in model predition.</p> <code>yy</code> <code>Array</code> <p>The y-coordinates of the patches in mesh-grid format</p> <code>xx</code> <code>Array</code> <p>The x-coordinates of the patches in mesh-grid format</p>"},{"location":"api/train/","title":"lacss.train","text":""},{"location":"api/train/#lacss.train.LacssTrainer","title":"<code>lacss.train.LacssTrainer</code>","text":"<p>             Bases: <code>Trainer</code></p> <p>Main trainer class for Lacss</p>"},{"location":"api/train/#lacss.train.lacss_trainer.LacssTrainer.__init__","title":"<code>__init__(config={}, collaborator_config=None, *, optimizer=None, seed=42, strategy=JIT)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>configuration dictionary for Lacss model</p> <code>{}</code> <code>collaborator_config</code> <code>Optional[dict]</code> <p>configuration dictionary for the collaborator model used in weakly-supervised training. If set to None, then no collaborator model will be created. In this case, training with weak-supervision will result in a error.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>seed</code> <code>Union[int, Array]</code> <p>RNG seed</p> <code>optimizer</code> <code>Optional[Optimizer]</code> <p>Override the default optimizer</p> <code>strategy</code> <code>type</code> <p>Training backend. See See Traing backends.</p>"},{"location":"api/train/#lacss.train.lacss_trainer.LacssTrainer.do_training","title":"<code>do_training(dataset, val_dataset=None, n_steps=50000, validation_interval=5000, checkpoint_manager=None, *, warmup_steps=0, sigma=20.0, pi=2.0)</code>","text":"<p>Runing training.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataSource</code> <p>An data iterator feed training data. The data should be in the form of a tuple: (x, y).     x is a dictionary with at least two keys:         \"image\": Trainging image.         \"gt_locations\": Point labels. Nx2 array     y is a dictionary of extra labels. It can be         None: for point-supervised training         \"gt_labels\": A index-label image (H, W). For segmentation label.         \"gt_image_mask\": A binary image (H, W). For weakly supervised training</p> required <code>n_steps</code> <code>int</code> <p>Total training steps</p> <code>50000</code> <code>validation_inteval</code> <p>Step intervals to perform validation and checkpointing.</p> required <code>val_dataset</code> <code>DataSource</code> <p>If not None, performing validation on this dataset. The data should be in the form of a tuple (x, y):     x is a dictionary with one key: \"image\"     y is a dictionary with two lables: \"gt_bboxes\" and \"gt_locations\"</p> <code>None</code> <code>checkpoint_manager</code> <code>Optional[orbax.checkpoint.CheckpointManager]</code> <p>If supplied, will be used to created checkpoints. A checkpoint manager can be obtained by calling:</p> <pre><code>options = orbax.CheckpointManagerOptions(...)\nmanager = orbax.checkpoint.CheckpointManager(\n    'path/to/directory/',\n    trainer.get_checkpointer(),\n    options = options\n)\n</code></pre> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>warmup_steps</code> <code>int</code> <p>Only used for point-supervised training. Pretraining steps, for which a large sigma values is used. This should be multiples of validation_inteval</p> <code>sigma</code> <code>float</code> <p>Only for point-supervised training. Expected cell size</p> <code>pi</code> <code>float</code> <p>Only for point-supervised training. Amplitude of the prior term.</p>"},{"location":"api/train/#lacss.train.lacss_trainer.LacssTrainer.from_checkpoint","title":"<code>from_checkpoint(cp_path)</code>  <code>classmethod</code>","text":"<p>load the module and its weight from a checkpoint     This utility static method allows use checkpoint as a model save. It ignores     optstate.</p> <p>Parameters:</p> Name Type Description Default <code>cp_path</code> <p>checkpoint location (a dir)</p> required <p>Returns:</p> Type Description <code>Tuple[nn.Module, Params]</code> <p>A tuple of module and weights.</p>"},{"location":"api/train/#lacss.train.lacss_trainer.LacssTrainer.get_checkpointer","title":"<code>get_checkpointer()</code>  <code>classmethod</code>","text":"<p>Returns a checkpointer obj for this Trainer. Convienent function for creating a checkpoint manager</p>"},{"location":"api/train/#lacss.train.lacss_trainer.LacssTrainer.pickle","title":"<code>pickle(save_path)</code>","text":"<p>Save a pickled copy of the Lacss model in the form of (model_config:dict, weights:FrozenDict). Only saves the principal model.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <p>Path to the pkl file</p> required"},{"location":"api/train/#lacss.train.lacss_trainer.LacssTrainer.restore_from_checkpoint","title":"<code>restore_from_checkpoint(checkpoint_manager, step=-1)</code>","text":"<p>Restore train state from a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_manager</code> <code>orbax.checkpoint.CheckpointManager</code> <p>Orbax checkpoint manager</p> required <code>step</code> <code>int</code> <p>The exact checkpoint to restore. Latest if unspecified.</p> <code>-1</code>"},{"location":"api/train/#lacss.train.Trainer","title":"<code>lacss.train.Trainer</code>","text":"<p>A general purpose FLAX model trainer. Help avoiding most of the biolerplate code when trainning with FLAX.</p> <p>Attributes:</p> Name Type Description <code>model</code> <p>A Flax module</p> <code>losses</code> <p>A list of loss function (or other callabels).</p> <code>optimizer</code> <p>An optax optimizer</p> <code>seed</code> <p>RNG seed</p> <code>params</code> <p>Current model parameters. This is a frozen dict. This is read/write.</p> <code>initialized</code> <p>Whether the model has been initialized with an optimizer and initial weights.</p>"},{"location":"api/train/#lacss.train.trainer.Trainer.__call__","title":"<code>__call__(inputs, *, strategy=None, **kwargs)</code>","text":"<p>Calling the underlying model</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Any</code> <p>A tuple or a dict as the inputs for the model.</p> required <code>strategy</code> <code>Optional[type]</code> <p>Optionally override the default strategy.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword args passed on to the model</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Model outputs.</p>"},{"location":"api/train/#lacss.train.trainer.Trainer.__init__","title":"<code>__init__(model, losses=None, optimizer=None, seed=42, strategy=strategy.JIT)</code>","text":"<p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>nn.Module</code> <p>A Flax module. It can be a bound module with parameters.</p> required <code>losses</code> <code>Optional[LOSSES]</code> <p>A loss function (callabels) or a list of loss functions, or None. These should have the call signiture of:     loss = loss_fn(inputs, labels, preds) The \"preds\" is the model output. The \"inputs\" and \"labels\" are from the train dataset. The model trains on the sum of all losses.</p> <code>None</code> <code>optimizer</code> <code>Optional[Optimizer]</code> <p>An optax optimzier</p> <code>None</code> <code>seed</code> <code>Union[int, Array]</code> <p>RNG seed.</p> <code>42</code> <code>strategy</code> <code>type</code> <p>Training backend. See Traing backends.</p> <code>strategy.JIT</code>"},{"location":"api/train/#lacss.train.trainer.Trainer.initialize","title":"<code>initialize(data, tx=None)</code>","text":"<p>Initialize the model weights and optimizer states.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>An iterator or generator function to produce training dataset. It is not used if model is bound with weights already. see train()</p> required <code>tx</code> <code>Optional[Optimizer]</code> <p>Optional optax optimzier for when the object was constructed without an optimizer</p> <code>None</code>"},{"location":"api/train/#lacss.train.trainer.Trainer.pickle","title":"<code>pickle(path)</code>","text":"<p>Make a pickle save of the trainer. This saves the model as well as the training states.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The file path.</p> required"},{"location":"api/train/#lacss.train.trainer.Trainer.reset","title":"<code>reset(loss_weights=None)</code>","text":"<p>Reset internal loss value tracking</p> <p>Parameters:</p> Name Type Description Default <code>loss_weights</code> <code>Optional[Sequence[float]]</code> <p>Optional weights of individual loss functions. If not None, the total loss is weighted sum.</p> <code>None</code>"},{"location":"api/train/#lacss.train.trainer.Trainer.restore_from_pickle","title":"<code>restore_from_pickle(path)</code>  <code>staticmethod</code>","text":"<p>Restore from the a pickled saved.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The pickle file path.</p> required <p>Returns:</p> Type Description <p>A new trainer object.</p>"},{"location":"api/train/#lacss.train.trainer.Trainer.test","title":"<code>test(dataset, metrics, strategy=None)</code>","text":"<p>Create test/validation iterator.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataSource</code> <p>An iterator or generator function to supply the testing data. The iterator should yield a tupple of (inputs, labels). The labels should be a dict.</p> required <code>metrics</code> <code>Callable</code> <p>A list of Metric objects. They should have two functions: m.update(preds, **kwargs):     preds is the model output. the remaining kwargs are content of     labels. m.compute():     which should return the accumulated metric value.</p> required <code>strategy</code> <code>Optional[type]</code> <p>Optionally override the default strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterator</code> <p>An iterator. Stepping through it will drive the updating of each metric obj. The iterator itself return the list of metrics.</p>"},{"location":"api/train/#lacss.train.trainer.Trainer.test_and_compute","title":"<code>test_and_compute(*args, **kwargs)</code>","text":"<p>A convient function to compute all metrics. See test() fucntion</p> <p>Returns:</p> Type Description <code>dict</code> <p>A metric dict. Keys are metric names.</p>"},{"location":"api/train/#lacss.train.trainer.Trainer.train","title":"<code>train(dataset, strategy=None, rng_cols=None, **kwargs)</code>","text":"<p>Create the training iterator</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataSource</code> <p>An iterator or generator function to supply the training data. The dataset should yield <code>(inputs, labels)</code> if the data come with labels or <code>(inputs, None)</code> if there is no label. The inputs is either a tuple or a dict. If the inputs is a dict, the keys are interpreted as the names for keyword args of the model's call function.</p> required <code>strategy</code> <code>Optional[type]</code> <p>Optionally override the default strategy.</p> <code>None</code> <code>rng_cols</code> <code>Sequence[str]</code> <p>Names of any RNG used by the model. Should be a list of strings.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyward args passed to the model. E.g. \"training=True\"</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator</code> <p>A iterator. Stepping through the iterator will train the model. The iterator itself returns a loss log dict, which are mean loss values for each loss function.</p> <p>Examples:</p> <pre><code>train_it = trainer.train(train_dataset, training=True)\nfor k in range(train_steps):\n    loss_logs = next(train_it)\n    if k % 1000 == 0:\n        print(loss_logs)\n        trainer.reset()\n</code></pre>"},{"location":"api/train/#training-backends","title":"Training backends","text":"class description lacss.train.Core No model JIT compiling, i.e., for debugging lacss.train.JIT JIT compile the model. Default strategy lacss.train.VMapped Transform the model with vmap. This allows defining a model on unbatched data but train with batched data. lacss.train.Distributed Transform the model with pmap. This allows training the model on multiple devices."}]}